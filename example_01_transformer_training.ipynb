{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "072ef70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0036640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from mytransformers.data import SimpleTranslationDataset\n",
    "from mytransformers.data import pad_to_seq_len\n",
    "from mytransformers.models import TransformerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384dd521",
   "metadata": {},
   "source": [
    "## training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b17672",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 256\n",
    "VOCAB_SIZE = 8000\n",
    "BATCH_SIZE = 32\n",
    "WARM_EPOCHS = 5\n",
    "COOL_EPOCHS = 35\n",
    "MAX_EPOCHS = 50\n",
    "INIT_LR = 0.0001\n",
    "MAX_LR = 0.001\n",
    "GRAD_CLIP = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fcbe45",
   "metadata": {},
   "source": [
    "## make datasets and dataloaders\n",
    "\n",
    "this is being trained for English > Korean translation on the `korean-english-news-v1` dataset from https://github.com/jungyeul/korean-parallel-corpora \n",
    "\n",
    "no preprocessing is done except to read in the data and write the (en, kr) pairs to tsv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eebbcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"data/translation_news/en-ko-train.tsv\"\n",
    "valid_file = \"data/translation_news/en-ko-dev.tsv\"\n",
    "train_tokenizer = \"data/translation_news/src_tokenizer.pkl\"\n",
    "valid_tokenizer = \"data/translation_news/tgt_tokenizer.pkl\"\n",
    "checkpoint_file = \"data/translation_news/checkpoint.pt\"\n",
    "sample_sentences = [\n",
    "    \"After keeping the world's most powerful supercomputer to themselves for a year, government researchers showed off the $110 million wonder and said it might help save the world from nuclear war.\",\n",
    "    \"Most of the people involved in the discussion agree that there is a legitimate area in which the government needs to retain the right to intercept communications.\",\n",
    "    \"Several Texan transmission companies announced Monday they were forming a consortium to invest in the $5 billion cost of building new power lines to take advantage of the state's vast wind power.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8727eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for training with the alternate 'bible' dataset.\n",
    "# # here, the verse notes are removed, and every tenth item starting from the second (index = 1) is set to validation\n",
    "# train_file = \"data/translation_bible/train_pairs.tsv\"\n",
    "# valid_file = \"data/translation_bible/valid_pairs.tsv\"\n",
    "# train_tokenizer = \"data/translation_bible/src_tokenizer.pkl\"\n",
    "# valid_tokenizer = \"data/translation_bible/tgt_tokenizer.pkl\"\n",
    "# checkpoint_file = \"data/translation_bible/checkpoint.pt\"\n",
    "# sample_sentences = [\n",
    "#     \"The weapons we fight with are not the weapons of the world. On the contrary, they have divine power to demolish strongholds.\",\n",
    "#     \"Make it your ambition to lead a quiet life, to mind your own business and to work with your hands, just as we told you,\",\n",
    "#     \"It had a great, high wall with twelve gates, and with twelve angels at the gates. On the gates were written the names of the twelve tribes of Israel.\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d89e2858",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting tokenizer...\n",
      "fitting source tokenizer...\n",
      "fitting target tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: \n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  control_symbols: [CLS]\n",
      "  control_symbols: [SEP]\n",
      "  control_symbols: [MASK]\n",
      "  control_symbols: [NEW1]\n",
      "  control_symbols: [NEW2]\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: [UNK]\n",
      "  bos_piece: [BOS]\n",
      "  eos_piece: [EOS]\n",
      "  pad_piece: [PAD]\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 94123 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [BOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [EOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [CLS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [SEP]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [MASK]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [NEW1]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [NEW2]\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=11967043\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.955% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=82\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.99955\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 94123 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 142416 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 94123\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 109507\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 109507 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=55808 obj=11.4654 num_tokens=233550 num_tokens/piece=4.18488\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=47027 obj=9.15743 num_tokens=234355 num_tokens/piece=4.98341\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=35267 obj=9.14063 num_tokens=247365 num_tokens/piece=7.01406\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=35257 obj=9.12308 num_tokens=247528 num_tokens/piece=7.02068\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=26442 obj=9.21581 num_tokens=267778 num_tokens/piece=10.127\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=26442 obj=9.19271 num_tokens=267762 num_tokens/piece=10.1264\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19831 obj=9.33007 num_tokens=291501 num_tokens/piece=14.6993\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19831 obj=9.30291 num_tokens=291468 num_tokens/piece=14.6976\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=14873 obj=9.48464 num_tokens=315871 num_tokens/piece=21.2379\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=14873 obj=9.4509 num_tokens=315857 num_tokens/piece=21.2369\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11154 obj=9.679 num_tokens=340152 num_tokens/piece=30.496\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11154 obj=9.63773 num_tokens=340115 num_tokens/piece=30.4926\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=9.85836 num_tokens=359983 num_tokens/piece=40.9072\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8800 obj=9.82062 num_tokens=360102 num_tokens/piece=40.9207\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: \n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  control_symbols: [CLS]\n",
      "  control_symbols: [SEP]\n",
      "  control_symbols: [MASK]\n",
      "  control_symbols: [NEW1]\n",
      "  control_symbols: [NEW2]\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: [UNK]\n",
      "  bos_piece: [BOS]\n",
      "  eos_piece: [EOS]\n",
      "  pad_piece: [PAD]\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 94123 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [BOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [EOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [CLS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [SEP]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [MASK]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [NEW1]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [NEW2]\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5811421\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1324\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 94123 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 176807 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 94123\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 241207\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 241207 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=95151 obj=14.8542 num_tokens=531109 num_tokens/piece=5.58175\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=83596 obj=13.516 num_tokens=533507 num_tokens/piece=6.38197\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=62692 obj=13.5434 num_tokens=554730 num_tokens/piece=8.8485\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=62663 obj=13.5039 num_tokens=555221 num_tokens/piece=8.86043\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46996 obj=13.6773 num_tokens=583777 num_tokens/piece=12.4218\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46995 obj=13.6361 num_tokens=584070 num_tokens/piece=12.4283\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=35246 obj=13.867 num_tokens=615085 num_tokens/piece=17.4512\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=35246 obj=13.8185 num_tokens=615078 num_tokens/piece=17.451\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=26434 obj=14.1044 num_tokens=647378 num_tokens/piece=24.4904\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=26434 obj=14.05 num_tokens=647384 num_tokens/piece=24.4906\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19825 obj=14.3784 num_tokens=680954 num_tokens/piece=34.3482\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19825 obj=14.3167 num_tokens=681044 num_tokens/piece=34.3528\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=14868 obj=14.6847 num_tokens=716521 num_tokens/piece=48.1922\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=14868 obj=14.6141 num_tokens=716522 num_tokens/piece=48.1922\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=11151 obj=15.0565 num_tokens=752674 num_tokens/piece=67.4983\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11151 obj=14.9758 num_tokens=752672 num_tokens/piece=67.4982\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=15.3823 num_tokens=782772 num_tokens/piece=88.9514\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8800 obj=15.3086 num_tokens=782842 num_tokens/piece=88.9593\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SimpleTranslationDataset(train_file, vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebb740e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer, tgt_tokenizer = train_dataset.get_tokenizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e862c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(src_tokenizer, open(train_tokenizer, \"wb\"))\n",
    "pickle.dump(tgt_tokenizer, open(valid_tokenizer, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c970636",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = SimpleTranslationDataset(valid_file, \n",
    "                                         src_tokenizer=src_tokenizer, \n",
    "                                         tgt_tokenizer=tgt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7df20326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples: 94123\n",
      "valid samples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"train samples: {}\".format(len(train_dataset)))\n",
    "print(\"valid samples: {}\".format(len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6470ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                                               collate_fn=partial(pad_to_seq_len, max_seq_len=MAX_SEQ_LEN))\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                                               collate_fn=partial(pad_to_seq_len, max_seq_len=MAX_SEQ_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed67c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_example = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6c3463b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32, 256])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for t in data_example:\n",
    "    print(t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099a2fc",
   "metadata": {},
   "source": [
    "## create model, etc\n",
    "\n",
    "the model configuration is loosely based on the *Attention is All You Need* base configuration, with the following changes:\n",
    "\n",
    "- the token embedding space used is smaller than the transformer input dimension, like ALBERT\n",
    "- a small amount of dropout is added to the first FFNN projection (`ffnn_dropout`)\n",
    "- the GELU activation is used in the FFNN layer, like BERT and GPT\n",
    "- the pre-layernorm configuration is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6924fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytransformer = TransformerModel(\n",
    "     src_vocab_sz=VOCAB_SIZE,\n",
    "     tgt_vocab_sz=VOCAB_SIZE,\n",
    "     enc_layers=6,\n",
    "     dec_layers=6,\n",
    "     seq_len=MAX_SEQ_LEN,\n",
    "     d_vocab=256,\n",
    "     d_in=512, \n",
    "     d_attn=64, \n",
    "     d_ffnn=1024, \n",
    "     attn_heads=8, \n",
    "     dropout=0.1,\n",
    "     attn_dropout=0.0, \n",
    "     ffnn_dropout=0.05,\n",
    "     pos_encoding=\"sinusoidal\",\n",
    "     shared_vocab=False,\n",
    "     attn_mask_val=-1e08, \n",
    "     ffnn_activation=\"gelu\", \n",
    "     pre_ln=True\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cd803f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_vocab_sz': 8000,\n",
       " 'tgt_vocab_sz': 8000,\n",
       " 'enc_layers': 6,\n",
       " 'dec_layers': 6,\n",
       " 'seq_len': 256,\n",
       " 'd_vocab': 256,\n",
       " 'd_in': 512,\n",
       " 'd_attn': 64,\n",
       " 'd_ffnn': 1024,\n",
       " 'attn_heads': 8,\n",
       " 'dropout': 0.1,\n",
       " 'attn_dropout': 0.0,\n",
       " 'ffnn_dropout': 0.05,\n",
       " 'pos_encoding': 'sinusoidal',\n",
       " 'shared_vocab': False,\n",
       " 'attn_mask_val': -100000000.0,\n",
       " 'attn_q_bias': False,\n",
       " 'attn_kv_bias': False,\n",
       " 'attn_out_bias': False,\n",
       " 'ffnn_activation': 'gelu',\n",
       " 'pre_ln': True}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytransformer.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4050278d",
   "metadata": {},
   "source": [
    "### learning rate scheduling\n",
    "\n",
    "we'll use the `OneCycleLR` to roughly approximate the warmup and annealing by 'warming up' for 1 epoch and then decaying for 49 epochs (until 50th epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7d9ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0, reduction=\"sum\")\n",
    "\n",
    "optimizer = torch.optim.Adam(mytransformer.parameters(), lr=INIT_LR, betas=(0.9, 0.98), eps=1e-09, weight_decay=0.0001, amsgrad=False)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                max_lr=MAX_LR, \n",
    "                                                total_steps=len(train_dataloader)*(WARM_EPOCHS+COOL_EPOCHS), \n",
    "                                                pct_start=WARM_EPOCHS/(WARM_EPOCHS+COOL_EPOCHS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1585c",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43aec1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 1 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2942/2942 [15:44<00:00,  3.12it/s, global_step=2942, loss=7.660]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:57:19.60] epoch 001 global step 2942: loss:    7.641\tavg:    7.660 (end of epoch)\n",
      "\n",
      "[19:57:20.60] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[19:57:25.18] epoch 001 eval loss:    7.643\n",
      "\n",
      "[19:57:25.18] checkpoint saved!\n",
      "starting epoch 2 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2942/2942 [15:45<00:00,  3.11it/s, global_step=5884, loss=6.213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:13:13.61] epoch 002 global step 5884: loss:    6.576\tavg:    6.213 (end of epoch)\n",
      "\n",
      "[20:13:14.61] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20:13:19.18] epoch 002 eval loss:    6.322\n",
      "\n",
      "[20:13:19.18] checkpoint saved!\n",
      "starting epoch 3 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2942/2942 [15:41<00:00,  3.13it/s, global_step=8826, loss=5.538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:29:02.79] epoch 003 global step 8826: loss:    5.752\tavg:    5.538 (end of epoch)\n",
      "\n",
      "[20:29:03.80] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20:29:08.37] epoch 003 eval loss:    5.521\n",
      "\n",
      "[20:29:08.37] checkpoint saved!\n",
      "starting epoch 4 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2942/2942 [15:40<00:00,  3.13it/s, global_step=11768, loss=5.189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:44:51.55] epoch 004 global step 11768: loss:    5.173\tavg:    5.189 (end of epoch)\n",
      "\n",
      "[20:44:52.55] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20:44:57.12] epoch 004 eval loss:    5.193\n",
      "\n",
      "[20:44:57.12] checkpoint saved!\n",
      "starting epoch 5 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2942/2942 [15:39<00:00,  3.13it/s, global_step=14710, loss=4.891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:00:39.34] epoch 005 global step 14710: loss:    4.541\tavg:    4.891 (end of epoch)\n",
      "\n",
      "[21:00:40.34] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[21:00:44.94] epoch 005 eval loss:    4.923\n",
      "\n",
      " sample greedy outputs:\n",
      "\n",
      "\tsrc: After keeping the world's most powerful supercomputer to themselves for a year, government researchers showed off the $110 million wonder and said it might help save the world from nuclear war.\n",
      "\tprd: 세계 최대 규모의 가장 높은 및 세계 최대 및 세계 및 및 세계 및 및 및 및 세계 및 및 및 세계 및 및 및 및 및 및 및 세계 및 및 및 및 및 세계적인 활동이 있다고 주장했다.\n",
      "\n",
      "\tsrc: Most of the people involved in the discussion agree that there is a legitimate area in which the government needs to retain the right to intercept communications.\n",
      "\tprd: 대부분의 최대 규모의 최대 규모는 정부의 주장이 있다고 주장했다.\n",
      "\n",
      "\tsrc: Several Texan transmission companies announced Monday they were forming a consortium to invest in the $5 billion cost of building new power lines to take advantage of the state's vast wind power.\n",
      "\tprd: Condordordordordordordordordordordordordordordordordordordordordordordordordordord in the theeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "\n",
      "\n",
      "[21:00:44.94] checkpoint saved!\n",
      "starting epoch 6 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2942/2942 [15:40<00:00,  3.13it/s, global_step=17652, loss=4.644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:16:30.92] epoch 006 global step 17652: loss:    4.496\tavg:    4.644 (end of epoch)\n",
      "\n",
      "[21:16:31.93] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[21:16:36.50] epoch 006 eval loss:    4.685\n",
      "\n",
      "[21:16:36.50] checkpoint saved!\n",
      "starting epoch 7 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2942/2942 [15:39<00:00,  3.13it/s, global_step=20594, loss=4.473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:32:18.75] epoch 007 global step 20594: loss:    4.404\tavg:    4.473 (end of epoch)\n",
      "\n",
      "[21:32:19.75] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[21:32:24.34] epoch 007 eval loss:    4.534\n",
      "\n",
      "[21:32:24.34] checkpoint saved!\n",
      "starting epoch 8 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2942/2942 [15:40<00:00,  3.13it/s, global_step=23536, loss=4.362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:48:07.79] epoch 008 global step 23536: loss:    4.471\tavg:    4.362 (end of epoch)\n",
      "\n",
      "[21:48:08.79] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[21:48:13.38] epoch 008 eval loss:    4.412\n",
      "\n",
      "[21:48:13.38] checkpoint saved!\n",
      "starting epoch 9 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2942/2942 [15:40<00:00,  3.13it/s, global_step=26478, loss=4.285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:03:56.89] epoch 009 global step 26478: loss:    4.180\tavg:    4.285 (end of epoch)\n",
      "\n",
      "[22:03:57.90] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[22:04:02.48] epoch 009 eval loss:    4.326\n",
      "\n",
      "[22:04:02.48] checkpoint saved!\n",
      "starting epoch 10 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2942/2942 [15:40<00:00,  3.13it/s, global_step=29420, loss=4.251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:19:45.81] epoch 010 global step 29420: loss:    4.394\tavg:    4.251 (end of epoch)\n",
      "\n",
      "[22:19:46.81] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[22:19:51.39] epoch 010 eval loss:    4.293\n",
      "\n",
      " sample greedy outputs:\n",
      "\n",
      "\tsrc: After keeping the world's most powerful supercomputer to themselves for a year, government researchers showed off the $110 million wonder and said it might help save the world from nuclear war.\n",
      "\tprd: 세계 최대의 세계인 세계 최대의 세계인 세계인 세계 에서 가장 많은 수가 있는 세계인 세계인 세계인의 한 지역은 세계가 세계를 지원할 수 있다고 밝혔다.\n",
      "\n",
      "\tsrc: Most of the people involved in the discussion agree that there is a legitimate area in which the government needs to retain the right to intercept communications.\n",
      "\tprd: 대부분의 사람들은 정부의 입장을 밝히지 않고 있는 정부를 구성하는 것이 합의를 위해 정부를 구성할 것이라고 주장했다.\n",
      "\n",
      "\tsrc: Several Texan transmission companies announced Monday they were forming a consortium to invest in the $5 billion cost of building new power lines to take advantage of the state's vast wind power.\n",
      "\tprd: 일부 기업들은 4일(현지시간) 투자은행의 투자은행을 투자하는 주택 투자은행으로 투자하는 주택 투자은행을 투자하는 데 도움이 될 것이라고 밝혔다.\n",
      "\n",
      "\n",
      "[22:19:51.39] checkpoint saved!\n",
      "starting epoch 11 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2942/2942 [15:40<00:00,  3.13it/s, global_step=32362, loss=4.246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:35:36.03] epoch 011 global step 32362: loss:    4.391\tavg:    4.246 (end of epoch)\n",
      "\n",
      "[22:35:37.03] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[22:35:41.62] epoch 011 eval loss:    4.251\n",
      "\n",
      "[22:35:41.62] checkpoint saved!\n",
      "starting epoch 12 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2942/2942 [15:39<00:00,  3.13it/s, global_step=35304, loss=4.187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:51:23.86] epoch 012 global step 35304: loss:    4.104\tavg:    4.187 (end of epoch)\n",
      "\n",
      "[22:51:24.86] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[22:51:29.45] epoch 012 eval loss:    4.215\n",
      "\n",
      "[22:51:29.45] checkpoint saved!\n",
      "starting epoch 13 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2942/2942 [15:39<00:00,  3.13it/s, global_step=38246, loss=4.073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:07:11.27] epoch 013 global step 38246: loss:    4.144\tavg:    4.073 (end of epoch)\n",
      "\n",
      "[23:07:12.27] evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:03<00:00,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[23:07:16.85] epoch 013 eval loss:    4.205\n",
      "\n",
      "[23:07:16.85] checkpoint saved!\n",
      "starting epoch 14 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch:  54%|██████████████████████████████████████████████████████████████████████▌                                                           | 1598/2942 [08:29<07:10,  3.12it/s, global_step=39844, loss=4.068]"
     ]
    }
   ],
   "source": [
    "mytransformer.train()\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "windowed_losses = []\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "\n",
    "    print(\"starting epoch {} of {}\".format(epoch+1, MAX_EPOCHS))\n",
    "    time.sleep(1)\n",
    "    \n",
    "    with tqdm.tqdm(train_dataloader, desc=\"minibatch\", total=len(train_dataloader)) as b:\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "\n",
    "            x, y_in, y_true, x_lens, y_lens = batch\n",
    "            x = x.to(\"cuda\")\n",
    "            y_in = y_in.to(\"cuda\")\n",
    "            y_true = y_true.to(\"cuda\")\n",
    "            x_lens = x_lens.to(\"cuda\")\n",
    "            y_lens = y_lens.to(\"cuda\")\n",
    "\n",
    "            y_pred = mytransformer(x, y_in, x_lens, y_lens)\n",
    "\n",
    "            loss = criterion(torch.transpose(y_pred, 1, 2), y_true)\n",
    "            loss /= torch.sum(y_lens)  # scale by all non-zero elements\n",
    "\n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(mytransformer.parameters(), GRAD_CLIP)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            windowed_losses.append(loss.item())\n",
    "            windowed_losses = windowed_losses[-16:]\n",
    "\n",
    "            global_step += 1\n",
    "            \n",
    "            b.set_postfix(loss=\"{:.3f}\".format(np.mean(windowed_losses)), global_step=global_step)\n",
    "            b.update(1)\n",
    "            \n",
    "    # end of epoch loss\n",
    "    tme = datetime.datetime.now().isoformat()[11:22]\n",
    "    print(\"[{}] epoch {:>03d} global step {:>04d}: loss: {:>8.3f}\\tavg: {:>8.3f} (end of epoch)\".format(\n",
    "        tme, epoch+1, global_step, loss.item(), np.mean(windowed_losses)\n",
    "    ))\n",
    "        \n",
    "    # evaluate\n",
    "    eval_losses = []\n",
    "    time.sleep(1)\n",
    "    tme = datetime.datetime.now().isoformat()[11:22]\n",
    "    print(\"\\n[{}] evaluating...\\n\".format(tme))\n",
    "    time.sleep(1)\n",
    "    \n",
    "    mytransformer.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(valid_dataloader):\n",
    "            x, y_in, y_true, x_lens, y_lens = batch\n",
    "            x = x.to(\"cuda\")\n",
    "            y_in = y_in.to(\"cuda\")\n",
    "            y_true = y_true.to(\"cuda\")\n",
    "            x_lens = x_lens.to(\"cuda\")\n",
    "            y_lens = y_lens.to(\"cuda\")\n",
    "            y_pred = mytransformer(x, y_in, x_lens, y_lens)\n",
    "            loss = criterion(torch.transpose(y_pred, 1, 2), y_true)\n",
    "            loss /= torch.sum(y_lens)  # scale by all non-zero elements\n",
    "            eval_losses.append(loss.item())\n",
    "            \n",
    "    tme = datetime.datetime.now().isoformat()[11:22]\n",
    "    print(\"\\n[{}] epoch {:>03d} eval loss: {:>8.3f}\".format(tme, epoch+1, np.mean(eval_losses)))\n",
    "    \n",
    "    # infer some results\n",
    "    if (epoch + 1) % 5 == 0 or (epoch + 1) == MAX_EPOCHS:\n",
    "        time.sleep(1)\n",
    "        print(\"\\n sample greedy outputs:\\n\")\n",
    "        with torch.no_grad():\n",
    "            for sample in sample_sentences:\n",
    "                x, x_len = src_tokenizer.transform(sample, as_array=True, bos=True, eos=True, max_len=MAX_SEQ_LEN)\n",
    "                x = torch.from_numpy(x).long().to(\"cuda\")\n",
    "                x_len = torch.from_numpy(x_len).long().to(\"cuda\")\n",
    "                y_hat = mytransformer.infer_one_greedy(x, x_len, bos=2, eos=3)\n",
    "                y_hat = tgt_tokenizer.inverse_transform([y_hat], as_tokens=False)[0]\n",
    "                print(\"\\tsrc: {}\".format(sample))\n",
    "                print(\"\\tprd: {}\\n\".format(y_hat))\n",
    "    \n",
    "    \n",
    "    # save\n",
    "    torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'global_step': global_step,\n",
    "            'model_state_dict': mytransformer.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'windowed_losses': windowed_losses,\n",
    "            'avg_loss': np.mean(windowed_losses),\n",
    "            'eval_loss': np.mean(eval_losses),\n",
    "            'training_config': mytransformer.config,\n",
    "            'batch_size': BATCH_SIZE\n",
    "            }, checkpoint_file)\n",
    "    \n",
    "    print(\"\\n[{}] checkpoint saved!\".format(tme))\n",
    "    \n",
    "    \n",
    "    mytransformer.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374d249b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
