# basic transformers

basic implementations of transformer model(s) for my own study

## current status & plans

- [x] multi-head attention
- [ ] sinusoidal positional encoding
- [ ] basic transformer encoder, from [*Attention is All You Need*](https://arxiv.org/abs/1706.03762)
- [ ] basic transformer decoder, from [*Attention is All You Need*](https://arxiv.org/abs/1706.03762)
- [ ] sentencepiece-based tokenizer
- [ ] basic seq2seq training and generation scripts (translation)
- [ ] GPT-style decoder and example, from [*Improving Language Understanding
by Generative Pre-Training*](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [ ] BERT-style encoder and example, from [*BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://arxiv.org/abs/1810.04805)
- [ ] ALBERT-style encoder and example, from [*ALBERT: A Lite BERT for Self-supervised Learning of Language Representations*](https://arxiv.org/abs/1909.11942)

## how to use

todo later

## references

### papers

[*Attention is All You Need*](https://arxiv.org/abs/1706.03762)  
[*Improving Language Understanding
by Generative Pre-Training*](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)  
[*BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*](https://arxiv.org/abs/1810.04805)  
[*ALBERT: A Lite BERT for Self-supervised Learning of Language Representations*](https://arxiv.org/abs/1909.11942)  

### reference implementations and articles

[pytorch `Transformer` documentation](https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer)  
[*The Illustrated Transformer*](https://jalammar.github.io/illustrated-transformer/)  
[*Transformers Explained Visually (Part 3): Multi-head Attention, deep dive*](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)  
[*How to code The Transformer in Pytorch*](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec#3fa3)  
github: [wzlxjtu/PositionalEncoding2D](https://github.com/wzlxjtu/PositionalEncoding2D)  